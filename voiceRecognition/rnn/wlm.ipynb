{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow & libraries \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.contrib.rnn import LSTMCell, MultiRNNCell, DropoutWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WLM models \n",
    "\n",
    "class ptb_wlm(object): \n",
    "    def __init__(self, seq_len, LSTM_dim = 250, n_label = 10000, dropout_ratio = 0.5, clip_norm = 1.0, name = 'ptb_wlm'):\n",
    "        self.seq_len = seq_len\n",
    "        self.name = name\n",
    "        self.LSTM_dim = LSTM_dim\n",
    "        self.n_label = n_label\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "\n",
    "        with tf.variable_scope(self.name):                                                                \n",
    "            ## constuct networks \n",
    "            \n",
    "            # placeholders \n",
    "            self.x = tf.placeholder(tf.int32, [None, self.seq_len], name = 'x') # input \n",
    "            self.y = tf.placeholder(tf.int32, [None, self.seq_len], name = 'y') # labels \n",
    "            self.phase = tf.placeholder(tf.bool, [], name = 'phase') # train or inference\n",
    "            self.lr = tf.placeholder(tf.float32, [], name = 'lr') # learning rate, for lr scheduling \n",
    "            self.state = tf.placeholder(tf.float32, [None, 2, None, self.LSTM_dim], name = 'state')\n",
    "            \n",
    "            ##\n",
    "            # construct model here\n",
    "            ##\n",
    "\n",
    "            # lstm_output: output of lstm layers \n",
    "            logit = tf.layers.dense(lstm_output, self.n_label)\n",
    "            self.predict = tf.argmax(logit, -1)\n",
    "            # softmax loss             \n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logit, labels = self.y)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "            self.loss = loss\n",
    "            \n",
    "            opt = tf.train.GradientDescentOptimizer(self.lr)\n",
    "            gradients, variables = zip(*opt.compute_gradients(loss))\n",
    "            gradients, _ = tf.clip_by_global_norm(gradients, clip_norm)\n",
    "            train_op = opt.apply_gradients(zip(gradients, variables))\n",
    "            self.train_op = train_op\n",
    "        \n",
    "            self.saver = tf.train.Saver()\n",
    "            \n",
    "            \n",
    "model = ptb_wlm(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- 0 / 100 --------\n",
      "train loss -  6.78307  | ppl -  882.775\n",
      "valid loss -  6.5555  | ppl -  703.102\n",
      "status :  save_param , training time :  29.919086694717407\n",
      "-------- 1 / 100 --------\n",
      "train loss -  6.58648  | ppl -  725.226\n",
      "valid loss -  6.53572  | ppl -  689.332\n",
      "status :  save_param , training time :  28.839855909347534\n",
      "-------- 2 / 100 --------\n",
      "train loss -  6.53509  | ppl -  688.898\n",
      "valid loss -  6.43803  | ppl -  625.173\n",
      "status :  save_param , training time :  27.04491949081421\n",
      "-------- 3 / 100 --------\n",
      "train loss -  6.41239  | ppl -  609.348\n",
      "valid loss -  6.29264  | ppl -  540.578\n",
      "status :  save_param , training time :  27.29948377609253\n",
      "-------- 4 / 100 --------\n",
      "train loss -  6.28777  | ppl -  537.952\n",
      "valid loss -  6.17401  | ppl -  480.109\n",
      "status :  save_param , training time :  28.188401460647583\n",
      "-------- 5 / 100 --------\n",
      "train loss -  6.1947  | ppl -  490.142\n",
      "valid loss -  6.09798  | ppl -  444.957\n",
      "status :  save_param , training time :  28.537468671798706\n",
      "-------- 6 / 100 --------\n",
      "train loss -  6.12046  | ppl -  455.076\n",
      "valid loss -  6.02737  | ppl -  414.622\n",
      "status :  save_param , training time :  27.850980281829834\n",
      "-------- 7 / 100 --------\n",
      "train loss -  6.05762  | ppl -  427.357\n",
      "valid loss -  5.97169  | ppl -  392.166\n",
      "status :  save_param , training time :  27.463860034942627\n",
      "-------- 8 / 100 --------\n",
      "train loss -  5.99779  | ppl -  402.539\n",
      "valid loss -  5.90721  | ppl -  367.681\n",
      "status :  save_param , training time :  26.84710693359375\n",
      "-------- 9 / 100 --------\n",
      "train loss -  5.93237  | ppl -  377.045\n",
      "valid loss -  5.8376  | ppl -  342.956\n",
      "status :  save_param , training time :  28.273735523223877\n",
      "-------- 10 / 100 --------\n",
      "train loss -  5.8665  | ppl -  353.011\n",
      "valid loss -  5.76239  | ppl -  318.107\n",
      "status :  save_param , training time :  28.471146821975708\n",
      "-------- 11 / 100 --------\n",
      "train loss -  5.79676  | ppl -  329.233\n",
      "valid loss -  5.69106  | ppl -  296.208\n",
      "status :  save_param , training time :  27.616657972335815\n",
      "-------- 12 / 100 --------\n",
      "train loss -  5.73062  | ppl -  308.161\n",
      "valid loss -  5.63405  | ppl -  279.792\n",
      "status :  save_param , training time :  27.449910879135132\n",
      "-------- 13 / 100 --------\n",
      "train loss -  5.67299  | ppl -  290.903\n",
      "valid loss -  5.58452  | ppl -  266.272\n",
      "status :  save_param , training time :  27.397204875946045\n",
      "-------- 14 / 100 --------\n",
      "train loss -  5.62452  | ppl -  277.139\n",
      "valid loss -  5.54205  | ppl -  255.202\n",
      "status :  save_param , training time :  27.45552635192871\n",
      "-------- 15 / 100 --------\n",
      "train loss -  5.58147  | ppl -  265.462\n",
      "valid loss -  5.50602  | ppl -  246.17\n",
      "status :  save_param , training time :  27.370102167129517\n",
      "-------- 16 / 100 --------\n",
      "train loss -  5.54461  | ppl -  255.854\n",
      "valid loss -  5.48249  | ppl -  240.445\n",
      "status :  save_param , training time :  27.28948163986206\n",
      "-------- 17 / 100 --------\n",
      "train loss -  5.51144  | ppl -  247.506\n",
      "valid loss -  5.4406  | ppl -  230.581\n",
      "status :  save_param , training time :  27.188692808151245\n",
      "-------- 18 / 100 --------\n",
      "train loss -  5.48148  | ppl -  240.203\n",
      "valid loss -  5.42481  | ppl -  226.968\n",
      "status :  save_param , training time :  28.127153635025024\n",
      "-------- 19 / 100 --------\n",
      "train loss -  5.45427  | ppl -  233.755\n",
      "valid loss -  5.40683  | ppl -  222.924\n",
      "status :  save_param , training time :  27.209066152572632\n",
      "-------- 20 / 100 --------\n",
      "train loss -  5.42882  | ppl -  227.88\n",
      "valid loss -  5.38704  | ppl -  218.556\n",
      "status :  save_param , training time :  27.133601665496826\n",
      "-------- 21 / 100 --------\n",
      "train loss -  5.40638  | ppl -  222.824\n",
      "valid loss -  5.35001  | ppl -  210.611\n",
      "status :  save_param , training time :  27.54400324821472\n",
      "-------- 22 / 100 --------\n",
      "train loss -  5.38429  | ppl -  217.956\n",
      "valid loss -  5.33839  | ppl -  208.177\n",
      "status :  save_param , training time :  27.005606174468994\n",
      "-------- 23 / 100 --------\n",
      "train loss -  5.36402  | ppl -  213.582\n",
      "valid loss -  5.31757  | ppl -  203.888\n",
      "status :  save_param , training time :  27.39189314842224\n",
      "-------- 24 / 100 --------\n",
      "train loss -  5.34449  | ppl -  209.451\n",
      "valid loss -  5.30347  | ppl -  201.033\n",
      "status :  save_param , training time :  26.423007249832153\n",
      "-------- 25 / 100 --------\n",
      "train loss -  5.32616  | ppl -  205.646\n",
      "valid loss -  5.29812  | ppl -  199.96\n",
      "status :  save_param , training time :  27.92164969444275\n",
      "-------- 26 / 100 --------\n",
      "train loss -  5.30917  | ppl -  202.183\n",
      "valid loss -  5.28344  | ppl -  197.046\n",
      "status :  save_param , training time :  26.646448135375977\n",
      "-------- 27 / 100 --------\n",
      "train loss -  5.29196  | ppl -  198.733\n",
      "valid loss -  5.2624  | ppl -  192.944\n",
      "status :  save_param , training time :  26.647387742996216\n",
      "-------- 28 / 100 --------\n",
      "train loss -  5.27622  | ppl -  195.628\n",
      "valid loss -  5.25384  | ppl -  191.3\n",
      "status :  save_param , training time :  26.652798891067505\n",
      "-------- 29 / 100 --------\n",
      "train loss -  5.26057  | ppl -  192.592\n",
      "valid loss -  5.23093  | ppl -  186.967\n",
      "status :  save_param , training time :  27.508901119232178\n",
      "-------- 30 / 100 --------\n",
      "train loss -  5.24523  | ppl -  189.659\n",
      "valid loss -  5.22167  | ppl -  185.243\n",
      "status :  save_param , training time :  27.63160753250122\n",
      "-------- 31 / 100 --------\n",
      "train loss -  5.23218  | ppl -  187.2\n",
      "valid loss -  5.21212  | ppl -  183.483\n",
      "status :  save_param , training time :  27.50036907196045\n",
      "-------- 32 / 100 --------\n",
      "train loss -  5.21717  | ppl -  184.412\n",
      "valid loss -  5.20393  | ppl -  181.985\n",
      "status :  save_param , training time :  26.179380655288696\n",
      "-------- 33 / 100 --------\n",
      "train loss -  5.20441  | ppl -  182.074\n",
      "valid loss -  5.19457  | ppl -  180.29\n",
      "status :  save_param , training time :  27.609354257583618\n",
      "-------- 34 / 100 --------\n",
      "train loss -  5.19238  | ppl -  179.896\n",
      "valid loss -  5.18287  | ppl -  178.193\n",
      "status :  save_param , training time :  28.18599247932434\n",
      "-------- 35 / 100 --------\n",
      "train loss -  5.18074  | ppl -  177.814\n",
      "valid loss -  5.17306  | ppl -  176.454\n",
      "status :  save_param , training time :  27.17113757133484\n",
      "-------- 36 / 100 --------\n",
      "train loss -  5.16828  | ppl -  175.613\n",
      "valid loss -  5.16488  | ppl -  175.017\n",
      "status :  save_param , training time :  26.764944791793823\n",
      "-------- 37 / 100 --------\n",
      "train loss -  5.15735  | ppl -  173.704\n",
      "valid loss -  5.15537  | ppl -  173.359\n",
      "status :  save_param , training time :  26.309058904647827\n",
      "-------- 38 / 100 --------\n",
      "train loss -  5.14613  | ppl -  171.765\n",
      "valid loss -  5.14906  | ppl -  172.27\n",
      "status :  save_param , training time :  26.685088872909546\n",
      "-------- 39 / 100 --------\n",
      "train loss -  5.13471  | ppl -  169.815\n",
      "valid loss -  5.14959  | ppl -  172.362\n",
      "status :  keep_train , training time :  25.755932807922363\n",
      "-------- 40 / 100 --------\n",
      "train loss -  5.12423  | ppl -  168.045\n",
      "valid loss -  5.12828  | ppl -  168.726\n",
      "status :  save_param , training time :  27.03239870071411\n",
      "-------- 41 / 100 --------\n",
      "train loss -  5.11299  | ppl -  166.166\n",
      "valid loss -  5.12337  | ppl -  167.9\n",
      "status :  save_param , training time :  26.488016843795776\n",
      "-------- 42 / 100 --------\n",
      "train loss -  5.1044  | ppl -  164.745\n",
      "valid loss -  5.11749  | ppl -  166.916\n",
      "status :  save_param , training time :  26.863947868347168\n",
      "-------- 43 / 100 --------\n",
      "train loss -  5.09419  | ppl -  163.072\n",
      "valid loss -  5.10975  | ppl -  165.628\n",
      "status :  save_param , training time :  26.38873839378357\n",
      "-------- 44 / 100 --------\n",
      "train loss -  5.08469  | ppl -  161.53\n",
      "valid loss -  5.10638  | ppl -  165.072\n",
      "status :  save_param , training time :  26.540194988250732\n",
      "-------- 45 / 100 --------\n",
      "train loss -  5.07569  | ppl -  160.082\n",
      "valid loss -  5.09529  | ppl -  163.252\n",
      "status :  save_param , training time :  26.60692524909973\n",
      "-------- 46 / 100 --------\n",
      "train loss -  5.06589  | ppl -  158.522\n",
      "valid loss -  5.08594  | ppl -  161.732\n",
      "status :  save_param , training time :  26.51193904876709\n",
      "-------- 47 / 100 --------\n",
      "train loss -  5.05627  | ppl -  157.004\n",
      "valid loss -  5.07967  | ppl -  160.722\n",
      "status :  save_param , training time :  26.808677434921265\n",
      "-------- 48 / 100 --------\n",
      "train loss -  5.04935  | ppl -  155.921\n",
      "valid loss -  5.06935  | ppl -  159.072\n",
      "status :  save_param , training time :  26.685781717300415\n",
      "-------- 49 / 100 --------\n",
      "train loss -  5.03927  | ppl -  154.357\n",
      "valid loss -  5.06932  | ppl -  159.067\n",
      "status :  save_param , training time :  26.92517852783203\n",
      "-------- 50 / 100 --------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss -  5.03175  | ppl -  153.201\n",
      "valid loss -  5.06884  | ppl -  158.99\n",
      "status :  save_param , training time :  26.93541979789734\n",
      "-------- 51 / 100 --------\n",
      "train loss -  5.02312  | ppl -  151.885\n",
      "valid loss -  5.05146  | ppl -  156.251\n",
      "status :  save_param , training time :  26.004714488983154\n",
      "-------- 52 / 100 --------\n",
      "train loss -  5.01431  | ppl -  150.553\n",
      "valid loss -  5.04999  | ppl -  156.02\n",
      "status :  save_param , training time :  27.0794415473938\n",
      "-------- 53 / 100 --------\n",
      "train loss -  5.00615  | ppl -  149.328\n",
      "valid loss -  5.04273  | ppl -  154.893\n",
      "status :  save_param , training time :  27.22673511505127\n",
      "-------- 54 / 100 --------\n",
      "train loss -  4.99932  | ppl -  148.312\n",
      "valid loss -  5.03795  | ppl -  154.154\n",
      "status :  save_param , training time :  26.808645725250244\n",
      "-------- 55 / 100 --------\n",
      "train loss -  4.99056  | ppl -  147.019\n",
      "valid loss -  5.03528  | ppl -  153.742\n",
      "status :  save_param , training time :  28.347527742385864\n",
      "-------- 56 / 100 --------\n",
      "train loss -  4.9835  | ppl -  145.984\n",
      "valid loss -  5.03157  | ppl -  153.173\n",
      "status :  save_param , training time :  26.941849946975708\n",
      "-------- 57 / 100 --------\n",
      "train loss -  4.97475  | ppl -  144.713\n",
      "valid loss -  5.0202  | ppl -  151.442\n",
      "status :  save_param , training time :  26.751341581344604\n",
      "-------- 58 / 100 --------\n",
      "train loss -  4.96768  | ppl -  143.693\n",
      "valid loss -  5.02116  | ppl -  151.587\n",
      "status :  keep_train , training time :  27.285526990890503\n",
      "-------- 59 / 100 --------\n",
      "train loss -  4.96138  | ppl -  142.79\n",
      "valid loss -  5.01509  | ppl -  150.67\n",
      "status :  save_param , training time :  26.687496423721313\n",
      "-------- 60 / 100 --------\n",
      "train loss -  4.9539  | ppl -  141.727\n",
      "valid loss -  5.00937  | ppl -  149.811\n",
      "status :  save_param , training time :  26.973778247833252\n",
      "-------- 61 / 100 --------\n",
      "train loss -  4.94671  | ppl -  140.712\n",
      "valid loss -  5.00617  | ppl -  149.331\n",
      "status :  save_param , training time :  27.64469075202942\n",
      "-------- 62 / 100 --------\n",
      "train loss -  4.93902  | ppl -  139.634\n",
      "valid loss -  5.00533  | ppl -  149.206\n",
      "status :  save_param , training time :  27.19274353981018\n",
      "-------- 63 / 100 --------\n",
      "train loss -  4.93217  | ppl -  138.68\n",
      "valid loss -  4.99701  | ppl -  147.969\n",
      "status :  save_param , training time :  26.421285390853882\n",
      "-------- 64 / 100 --------\n",
      "train loss -  4.92577  | ppl -  137.795\n",
      "valid loss -  4.98588  | ppl -  146.332\n",
      "status :  save_param , training time :  26.670225858688354\n",
      "-------- 65 / 100 --------\n",
      "train loss -  4.91798  | ppl -  136.726\n",
      "valid loss -  4.98654  | ppl -  146.429\n",
      "status :  keep_train , training time :  27.665169954299927\n",
      "-------- 66 / 100 --------\n",
      "train loss -  4.9135  | ppl -  136.114\n",
      "valid loss -  4.98345  | ppl -  145.977\n",
      "status :  save_param , training time :  26.888895750045776\n",
      "-------- 67 / 100 --------\n",
      "train loss -  4.90692  | ppl -  135.222\n",
      "valid loss -  4.97961  | ppl -  145.418\n",
      "status :  save_param , training time :  26.556029319763184\n",
      "-------- 68 / 100 --------\n",
      "train loss -  4.90001  | ppl -  134.29\n",
      "valid loss -  4.97179  | ppl -  144.284\n",
      "status :  save_param , training time :  26.99554991722107\n",
      "-------- 69 / 100 --------\n",
      "train loss -  4.89257  | ppl -  133.295\n",
      "valid loss -  4.9737  | ppl -  144.561\n",
      "status :  keep_train , training time :  26.763630628585815\n",
      "-------- 70 / 100 --------\n",
      "train loss -  4.88636  | ppl -  132.47\n",
      "valid loss -  4.96888  | ppl -  143.866\n",
      "status :  save_param , training time :  27.281166553497314\n",
      "-------- 71 / 100 --------\n",
      "train loss -  4.88065  | ppl -  131.716\n",
      "valid loss -  4.96593  | ppl -  143.441\n",
      "status :  save_param , training time :  25.628652334213257\n",
      "-------- 72 / 100 --------\n",
      "train loss -  4.87437  | ppl -  130.892\n",
      "valid loss -  4.96058  | ppl -  142.676\n",
      "status :  save_param , training time :  27.05196452140808\n",
      "-------- 73 / 100 --------\n",
      "train loss -  4.86803  | ppl -  130.064\n",
      "valid loss -  4.9558  | ppl -  141.996\n",
      "status :  save_param , training time :  26.492992639541626\n",
      "-------- 74 / 100 --------\n",
      "train loss -  4.86229  | ppl -  129.32\n",
      "valid loss -  4.95102  | ppl -  141.319\n",
      "status :  save_param , training time :  27.427085638046265\n",
      "-------- 75 / 100 --------\n",
      "train loss -  4.85669  | ppl -  128.598\n",
      "valid loss -  4.94266  | ppl -  140.142\n",
      "status :  save_param , training time :  26.601661205291748\n",
      "-------- 76 / 100 --------\n",
      "train loss -  4.85018  | ppl -  127.764\n",
      "valid loss -  4.944  | ppl -  140.33\n",
      "status :  keep_train , training time :  27.30009126663208\n",
      "-------- 77 / 100 --------\n",
      "train loss -  4.845  | ppl -  127.104\n",
      "valid loss -  4.94254  | ppl -  140.126\n",
      "status :  save_param , training time :  25.68813920021057\n",
      "-------- 78 / 100 --------\n",
      "train loss -  4.83801  | ppl -  126.218\n",
      "valid loss -  4.94514  | ppl -  140.49\n",
      "status :  keep_train , training time :  27.037299871444702\n",
      "-------- 79 / 100 --------\n",
      "train loss -  4.83369  | ppl -  125.674\n",
      "valid loss -  4.94713  | ppl -  140.77\n",
      "status :  keep_train , training time :  26.700769424438477\n",
      "-------- 80 / 100 --------\n",
      "train loss -  4.8284  | ppl -  125.011\n",
      "valid loss -  4.93454  | ppl -  139.009\n",
      "status :  save_param , training time :  26.246570110321045\n",
      "-------- 81 / 100 --------\n",
      "train loss -  4.82308  | ppl -  124.348\n",
      "valid loss -  4.9343  | ppl -  138.976\n",
      "status :  save_param , training time :  26.712098836898804\n",
      "-------- 82 / 100 --------\n",
      "train loss -  4.81829  | ppl -  123.753\n",
      "valid loss -  4.92371  | ppl -  137.512\n",
      "status :  save_param , training time :  26.817979335784912\n",
      "-------- 83 / 100 --------\n",
      "train loss -  4.81275  | ppl -  123.07\n",
      "valid loss -  4.92717  | ppl -  137.989\n",
      "status :  keep_train , training time :  27.079774856567383\n",
      "-------- 84 / 100 --------\n",
      "train loss -  4.8064  | ppl -  122.29\n",
      "valid loss -  4.9181  | ppl -  136.743\n",
      "status :  save_param , training time :  27.480677604675293\n",
      "-------- 85 / 100 --------\n",
      "train loss -  4.80153  | ppl -  121.697\n",
      "valid loss -  4.9216  | ppl -  137.222\n",
      "status :  keep_train , training time :  27.328038930892944\n",
      "-------- 86 / 100 --------\n",
      "train loss -  4.79661  | ppl -  121.099\n",
      "valid loss -  4.91591  | ppl -  136.443\n",
      "status :  save_param , training time :  27.167648792266846\n",
      "-------- 87 / 100 --------\n",
      "train loss -  4.79247  | ppl -  120.599\n",
      "valid loss -  4.92066  | ppl -  137.093\n",
      "status :  keep_train , training time :  25.965326070785522\n",
      "-------- 88 / 100 --------\n",
      "train loss -  4.78629  | ppl -  119.856\n",
      "valid loss -  4.91268  | ppl -  136.003\n",
      "status :  save_param , training time :  27.04036235809326\n",
      "-------- 89 / 100 --------\n",
      "train loss -  4.78323  | ppl -  119.49\n",
      "valid loss -  4.91422  | ppl -  136.213\n",
      "status :  keep_train , training time :  26.183533430099487\n",
      "-------- 90 / 100 --------\n",
      "train loss -  4.77738  | ppl -  118.792\n",
      "valid loss -  4.91301  | ppl -  136.049\n",
      "status :  keep_train , training time :  26.328038454055786\n",
      "-------- 91 / 100 --------\n",
      "train loss -  4.77277  | ppl -  118.246\n",
      "valid loss -  4.90234  | ppl -  134.604\n",
      "status :  save_param , training time :  27.235006093978882\n",
      "-------- 92 / 100 --------\n",
      "train loss -  4.768  | ppl -  117.684\n",
      "valid loss -  4.9041  | ppl -  134.841\n",
      "status :  keep_train , training time :  26.00952410697937\n",
      "-------- 93 / 100 --------\n",
      "train loss -  4.76344  | ppl -  117.148\n",
      "valid loss -  4.90208  | ppl -  134.569\n",
      "status :  save_param , training time :  27.03171467781067\n",
      "-------- 94 / 100 --------\n",
      "train loss -  4.76049  | ppl -  116.804\n",
      "valid loss -  4.89917  | ppl -  134.178\n",
      "status :  save_param , training time :  26.487804412841797\n",
      "-------- 95 / 100 --------\n",
      "train loss -  4.75498  | ppl -  116.162\n",
      "valid loss -  4.89839  | ppl -  134.074\n",
      "status :  save_param , training time :  26.22569727897644\n",
      "-------- 96 / 100 --------\n",
      "train loss -  4.74874  | ppl -  115.438\n",
      "valid loss -  4.89758  | ppl -  133.965\n",
      "status :  save_param , training time :  26.21893072128296\n",
      "-------- 97 / 100 --------\n",
      "train loss -  4.74624  | ppl -  115.151\n",
      "valid loss -  4.89191  | ppl -  133.208\n",
      "status :  save_param , training time :  26.059911727905273\n",
      "-------- 98 / 100 --------\n",
      "train loss -  4.74039  | ppl -  114.479\n",
      "valid loss -  4.89343  | ppl -  133.411\n",
      "status :  keep_train , training time :  26.269075870513916\n",
      "-------- 99 / 100 --------\n",
      "train loss -  4.73664  | ppl -  114.051\n",
      "valid loss -  4.891  | ppl -  133.086\n",
      "status :  save_param , training time :  27.44774580001831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------final result for test data--------\n",
      "loss -  4.83679  | ppl -  126.064\n",
      "test set inference time :  1.9982125759124756\n"
     ]
    }
   ],
   "source": [
    "## training code \n",
    "import time \n",
    "\n",
    "#fix random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "#model parameters\n",
    "lstm_dim = 256\n",
    "dr_ratio = 0.5\n",
    "\n",
    "#for early stopping technique\n",
    "initial_lr = 1.0\n",
    "decay_factor = 0.5\n",
    "decay_time = 3\n",
    "patience = 2\n",
    "max_epoch = 100\n",
    "\n",
    "#batch size for train & evaluate\n",
    "train_batch = 50\n",
    "test_batch = 20\n",
    "sequence_length = 30\n",
    "\n",
    "#paths for data\n",
    "data_path = './ptb_data/'\n",
    "result_path = './results/'\n",
    "model_name = 'ptb_wlm'\n",
    "\n",
    "result_file = result_path + 'check_point_' + model_name\n",
    "\n",
    "\n",
    "#internal args for early stopping\n",
    "cur_lr = initial_lr\n",
    "cur_patience = 0\n",
    "cur_decay_time = 0\n",
    "best_valid_loss = 10000\n",
    "train_phase = True\n",
    "eval_phase = False\n",
    "\n",
    "train_loss_hist = []\n",
    "valid_loss_hist = []\n",
    "\n",
    "#load dataset\n",
    "from dataset import PTBDataset\n",
    "dataset = PTBDataset(train_batch, sequence_length, data_path = data_path, seed = seed)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    # graph construct\n",
    "    model = ptb_wlm(\n",
    "            name = model_name,\n",
    "            seq_len = sequence_length,\n",
    "            LSTM_dim = lstm_dim,\n",
    "            dropout_ratio = dr_ratio,\n",
    "            )\n",
    "    #set gpu usage to allow_growth\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "\n",
    "    #begin training\n",
    "    epoch = 0\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        status = 'keep_train'\n",
    "        #curriculum learning control\n",
    "        while epoch < max_epoch:\n",
    "            start_time = time.time()\n",
    "\n",
    "            #early stopping control\n",
    "            if status == 'end_train':\n",
    "                time.sleep(1)\n",
    "                model.saver.restore(sess, result_file)\n",
    "                model.saver.save(sess, result_path + model_name)\n",
    "                break\n",
    "            elif status == 'change_lr':\n",
    "                time.sleep(1)\n",
    "                model.saver.restore(sess, result_file)\n",
    "                cur_lr *= decay_factor\n",
    "                cur_patience = 0\n",
    "                cur_decay_time +=1\n",
    "                \n",
    "                print('lr changed to : ', cur_lr)\n",
    "                print('current decay : ', cur_decay_time, \" / \", decay_time)\n",
    "            elif status == 'save_param':\n",
    "                cur_patience = 0\n",
    "                model.saver.save(sess, result_file)\n",
    "            elif status == 'keep_train':\n",
    "                cur_patience +=1\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "\n",
    "            print('--------', epoch, '/', max_epoch, '--------')\n",
    "            #train\n",
    "            dataset.set_batch_size(train_batch)\n",
    "            dataset.set_mode('train')\n",
    "            epoch_loss = []\n",
    "            epoch_cer = []\n",
    "            with tf.name_scope('train'):\n",
    "                init_state = np.zeros([2, 2, train_batch, lstm_dim], dtype = 'float32')\n",
    "                \n",
    "                while dataset.iter_flag():\n",
    "                    batch_x, batch_y = dataset.get_data()\n",
    "                    cur_loss, last_state, _ = sess.run(\n",
    "                        [model.loss, model.last_state, model.train_op],\n",
    "                        feed_dict = {model.x: batch_x,\n",
    "                                     model.y: batch_y,\n",
    "                                     model.phase: train_phase,\n",
    "                                     model.state: init_state, \n",
    "                                     model.lr: cur_lr}\n",
    "                    )\n",
    "                    init_state = last_state                    \n",
    "                    epoch_loss.append(cur_loss)\n",
    "\n",
    "                epoch_loss = np.mean(np.asarray(epoch_loss, dtype='float32'))\n",
    "                train_loss_hist.append(epoch_loss)\n",
    "\n",
    "            # evaluation\n",
    "            epoch_loss = []\n",
    "            dataset.set_batch_size(test_batch)\n",
    "            dataset.set_mode('valid')\n",
    "\n",
    "            with tf.name_scope('valid'):\n",
    "                init_state = np.zeros([2, 2, test_batch, lstm_dim], dtype = 'float32')\n",
    "                while dataset.iter_flag():\n",
    "                    \n",
    "                    batch_x, batch_y = dataset.get_data()\n",
    "                    cur_loss, last_state = sess.run(\n",
    "                        [model.loss ,model.last_state],\n",
    "                        feed_dict={model.x: batch_x,\n",
    "                                   model.y: batch_y,\n",
    "                                   model.state: init_state,\n",
    "                                   model.phase: eval_phase}\n",
    "                    )\n",
    "                    init_state = last_state\n",
    "                    epoch_loss.append(cur_loss)\n",
    "                    continue\n",
    "\n",
    "                epoch_loss = np.mean(np.asarray(epoch_loss, dtype='float32'))\n",
    "                valid_loss_hist.append(epoch_loss)\n",
    "\n",
    "            #early stopping\n",
    "            if epoch_loss >= best_valid_loss:\n",
    "                if cur_patience == patience:\n",
    "                    if cur_decay_time == decay_time:\n",
    "                        status = 'end_train'\n",
    "                    else:\n",
    "                        status = 'change_lr'\n",
    "                else:\n",
    "                    status = 'keep_train'\n",
    "            else:\n",
    "                status = 'save_param'\n",
    "                best_valid_loss = epoch_loss\n",
    "\n",
    "            end_time = time.time()\n",
    "\n",
    "\n",
    "            print('train loss - ', train_loss_hist[-1], ' | ppl - ', np.exp(train_loss_hist[-1]))\n",
    "            print('valid loss - ', valid_loss_hist[-1], ' | ppl - ', np.exp(valid_loss_hist[-1]))\n",
    "            print('status : ', status, ', training time : ', end_time - start_time)\n",
    "            epoch+=1\n",
    "\n",
    "\n",
    "        #final test\n",
    "        # evaluation\n",
    "        epoch_loss = []\n",
    "        dataset.set_batch_size(test_batch)\n",
    "        dataset.set_mode('test')\n",
    "        start_time = time.time()\n",
    "        with tf.name_scope('test'):\n",
    "            init_state = np.zeros([2, 2, test_batch, lstm_dim], dtype = 'float32')\n",
    "            while dataset.iter_flag():\n",
    "                batch_x, batch_y = dataset.get_data()\n",
    "                cur_loss, last_state = sess.run(\n",
    "                    [model.loss, model.last_state],\n",
    "                    feed_dict={model.x: batch_x,\n",
    "                               model.y: batch_y,\n",
    "                               model.state: init_state,\n",
    "                               model.phase: eval_phase}\n",
    "                )\n",
    "                init_state = last_state\n",
    "                epoch_loss.append(cur_loss)\n",
    "            epoch_loss = np.mean(np.asarray(epoch_loss, dtype='float32'))\n",
    "\n",
    "        end_time = time.time()\n",
    "        print('\\n\\n--------final result for test data--------')\n",
    "        print('loss - ', epoch_loss, ' | ppl - ', np.exp(epoch_loss))\n",
    "        print('test set inference time : ', end_time - start_time)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./results/check_point_ptb_wlm\n",
      "<unk>\n",
      "'s\n",
      "it\n",
      "<unk>\n",
      "of\n",
      "-----generated ids-----\n",
      "[0, 37, 15, 6, 2079, 4, 0, 1, 4, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "-----generated sentences-----\n",
      "the company said a word of the <unk> of the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n"
     ]
    }
   ],
   "source": [
    "## generate \n",
    "\n",
    "#initial 5 words\n",
    "init_words = \"the company said a word\"\n",
    "word_to_id = dataset.word_to_id\n",
    "id_to_word = dataset.id_to_word\n",
    "init_ids = []\n",
    "for i in range(5):\n",
    "    init_ids.append(word_to_id[init_words.split(' ')[i]])\n",
    "gen_words = init_words\n",
    "gen_ids = init_ids\n",
    "\n",
    "init_state = np.zeros([2, 2, 1, lstm_dim], dtype = 'float32')\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    model = ptb_wlm(\n",
    "            name = model_name,\n",
    "            seq_len = 1,\n",
    "            LSTM_dim = lstm_dim,\n",
    "            dropout_ratio = dr_ratio,\n",
    "            )\n",
    "    #set gpu usage to allow_growth\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    \n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        model.saver.restore(sess, result_file)\n",
    "        for i in range(5):\n",
    "            cur_x = np.reshape(np.asarray([init_ids[i]], dtype = 'int32'), [1,1])\n",
    "            cur_predict, last_state = sess.run([model.predict, model.last_state],\n",
    "                    feed_dict = {\n",
    "                        model.x : cur_x,\n",
    "                        model.state: init_state,\n",
    "                        model.phase: eval_phase\n",
    "                        }\n",
    "                    )\n",
    "            init_state = last_state\n",
    "            \n",
    "            print(id_to_word[cur_predict[0][0]])\n",
    "        gen_ids.append(cur_predict[0][0])\n",
    "        gen_words = gen_words + ' ' + id_to_word[gen_ids[-1]]\n",
    "        \n",
    "        counter = 5\n",
    "        while gen_words.split(' ')[-1] != '<eos>' and counter < 50:\n",
    "            cur_x = np.reshape(np.asarray([gen_ids[-1]], dtype = 'int32'), [1,1])\n",
    "            cur_predict, last_state = sess.run([model.predict, model.last_state],\n",
    "                    feed_dict = {\n",
    "                        model.x : cur_x,\n",
    "                        model.state: init_state,\n",
    "                        model.phase: eval_phase\n",
    "                        }\n",
    "                    )\n",
    "            init_state = last_state\n",
    "            gen_ids.append(cur_predict[0][0])\n",
    "            gen_words = gen_words + ' ' + id_to_word[gen_ids[-1]]\n",
    "            counter+=1\n",
    "\n",
    "        print('-----generated ids-----')\n",
    "        print(gen_ids)\n",
    "        print('-----generated sentences-----')\n",
    "        print(gen_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
