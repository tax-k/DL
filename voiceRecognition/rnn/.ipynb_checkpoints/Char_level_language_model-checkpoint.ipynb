{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import tensorflow & libraries \n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.contrib.rnn import LSTMCell, MultiRNNCell, DropoutWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##clm Models\n",
    "\n",
    "class ptb_clm(object): \n",
    "    def __init__(self, seq_len, LSTM_dim = 250, n_label = 10000, dropout_ratio=0.5, clip_norm = 1.0, name='ptb_clm'):\n",
    "        self.seq_len = seq_len\n",
    "        self.name = name\n",
    "        self.LSTM_dim = LSTM_dim\n",
    "        self.n_label = n_label\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        \n",
    "        with tf.variable_scope(self.name):\n",
    "            ## constuct networks\n",
    "            \n",
    "            ##placeholders\n",
    "            self.x = tf.placeholder(tf.int32, [None, self.seq_len], name = 'x') # input\n",
    "            self.y = tf.placeholder(tf.int32, [None, self.seq_len], name = 'y') # labels\n",
    "            self.phase = tf.placeholder(tf.bool, [], name = 'phase') # train or inference\n",
    "            self.lr = tf.placeholder(tf.float32, [], name = 'lr') # learning for lr scheduling\n",
    "            self.state = tf.placeholder(tf.float32, [None, 2, None, self.LSTM_dim], name = 'state')\n",
    "            \n",
    "            \n",
    "            #np.zeors = 3, ~~\n",
    "            #wlm 에서는 lstm 이 2개\n",
    "            #clm 에서는 lstm 층이 3개\n",
    "            onehot_x = tf.one_hot(\n",
    "                self.x,\n",
    "                self.n_label,\n",
    "                axis = -1\n",
    "            )\n",
    "    \n",
    "            # lstm cells\n",
    "            lstm0 = LSTMCell(self.LSTM_dim)\n",
    "            lstm1 = LSTMCell(self.LSTM_dim)\n",
    "            lstm2 = LSTMCell(self.LSTM_dim)\n",
    "            \n",
    "            #dropout\n",
    "            #if training, keep_prob = 1 - dropout_ratio\n",
    "            #else, keep_prob = 1\n",
    "            keep_prob = tf.cond(self.phase, lambda:tf.constant((1.0-self.dropout_ratio)), lambda:tf.constant(1.0))\n",
    "            lstm0 = DropoutWrapper(lstm0, output_keep_prob = keep_prob)\n",
    "            lstm1 = DropoutWrapper(lstm1, output_keep_prob = keep_prob)\n",
    "            lstm2 = DropoutWrapper(lstm2, output_keep_prob = keep_prob)\n",
    "            #now dropout is applied to LSTMCells\n",
    "            #print(self.lstm0)\n",
    "            #print(self.lstm0.wrapped_cell) # may not work in old tensorflow \n",
    "            # signle LSTM layer\n",
    "            #lstm_output, last_state = tf.nn.dynamic_rnn(lstm0, word_embedding)\n",
    "            \n",
    "            # merge LSTM Cells to single RNN Cell (dynamic_rnn can call only one RNNCell)\n",
    "            lstm_cells = MultiRNNCell([lstm0, lstm1])\n",
    "            #print(lstm_cells)\n",
    "            initial_state = ( tf.nn.rnn_cell.LSTMStateTuple(self.state[0,0,: ,:], self.state[0,1,:,:]),\n",
    "                             tf.nn.rnn_cell.LSTMStateTuple(self.state[1,0,:,:], self.state[1,1,:,:]))\n",
    "            \n",
    "            lstm_output, last_state = tf.nn.dynamic_rnn(lstm_cells, onehot_x, initial_state=initial_state, dtype=tf.float32)\n",
    "            self.last_state = last_state\n",
    "            # or, call dynamic_rnn twice as follow: (not recommended) \n",
    "            #lstm_output, last_state0 = tf.nn.dynamic_rnn(lstm0, word_embedding)\n",
    "            #lstm_output, last_state1 = tf.nn.dynamic_rnn(lstm1, lstm_output)\n",
    "            #last_state = (last_state0, last_state1)\n",
    "            \n",
    "            logit = tf.layers.dense(lstm_output, self.n_label)\n",
    "            self.predict = tf.argmax(logit, -1)\n",
    "            # softmax loss             \n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logit, labels = self.y)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "            self.loss = loss\n",
    "            \n",
    "            opt = tf.train.GradientDescentOptimizer(self.lr)\n",
    "            gradients, variables = zip(*opt.compute_gradients(loss))\n",
    "            gradients, _ = tf.clip_by_global_norm(gradients, clip_norm)\n",
    "            train_op = opt.apply_gradients(zip(gradients, variables))\n",
    "            self.train_op = train_op\n",
    "        \n",
    "            self.saver = tf.train.Saver()\n",
    "            \n",
    "            \n",
    "model = ptb_clm(20)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training code \n",
    "import time \n",
    "\n",
    "#fix random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "#model parameters\n",
    "lstm_dim = 512\n",
    "dr_ratio = 0.5\n",
    "\n",
    "#for early stopping technique\n",
    "initial_lr = 1.0\n",
    "decay_factor = 0.5\n",
    "decay_time = 3\n",
    "patience = 2\n",
    "max_epoch = 100\n",
    "\n",
    "#batch size for train & evaluate\n",
    "train_batch = 50\n",
    "test_batch = 20\n",
    "sequence_length = 30\n",
    "\n",
    "#paths for data\n",
    "data_path = './ptb_data/'\n",
    "result_path = './results/'\n",
    "model_name = 'ptb_clm'\n",
    "\n",
    "result_file = result_path + 'check_point_' + model_name\n",
    "\n",
    "\n",
    "#internal args for early stopping\n",
    "cur_lr = initial_lr\n",
    "cur_patience = 0\n",
    "cur_decay_time = 0\n",
    "best_valid_loss = 10000\n",
    "train_phase = True\n",
    "eval_phase = False\n",
    "\n",
    "train_loss_hist = []\n",
    "valid_loss_hist = []\n",
    "\n",
    "#load dataset\n",
    "from dataset import PTBDataset\n",
    "dataset = PTBDataset(train_batch, sequence_length, data_path = data_path, seed = seed)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    # graph construct\n",
    "    model = ptb_clm(\n",
    "            name = model_name,\n",
    "            seq_len = sequence_length,\n",
    "            LSTM_dim = lstm_dim,\n",
    "            dropout_ratio = dr_ratio,\n",
    "            )\n",
    "    #set gpu usage to allow_growth\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "\n",
    "    #begin training\n",
    "    epoch = 0\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        status = 'keep_train'\n",
    "        #curriculum learning control\n",
    "        while epoch < max_epoch:\n",
    "            start_time = time.time()\n",
    "\n",
    "            #early stopping control\n",
    "            if status == 'end_train':\n",
    "                time.sleep(1)\n",
    "                model.saver.restore(sess, result_file)\n",
    "                model.saver.save(sess, result_path + model_name)\n",
    "                break\n",
    "            elif status == 'change_lr':\n",
    "                time.sleep(1)\n",
    "                model.saver.restore(sess, result_file)\n",
    "                cur_lr *= decay_factor\n",
    "                cur_patience = 0\n",
    "                cur_decay_time +=1\n",
    "                \n",
    "                print('lr changed to : ', cur_lr)\n",
    "                print('current decay : ', cur_decay_time, \" / \", decay_time)\n",
    "            elif status == 'save_param':\n",
    "                cur_patience = 0\n",
    "                model.saver.save(sess, result_file)\n",
    "            elif status == 'keep_train':\n",
    "                cur_patience +=1\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "\n",
    "            print('--------', epoch, '/', max_epoch, '--------')\n",
    "            #train\n",
    "            dataset.set_batch_size(train_batch)\n",
    "            dataset.set_mode('train')\n",
    "            epoch_loss = []\n",
    "            epoch_cer = []\n",
    "            with tf.name_scope('train'):\n",
    "                init_state = np.zeros([3, 2, train_batch, lstm_dim], dtype = 'float32')\n",
    "                \n",
    "                while dataset.iter_flag():\n",
    "                    batch_x, batch_y = dataset.get_data()\n",
    "                    cur_loss, last_state, _ = sess.run(\n",
    "                        [model.loss, model.last_state, model.train_op],\n",
    "                        feed_dict = {model.x: batch_x,\n",
    "                                     model.y: batch_y,\n",
    "                                     model.phase: train_phase,\n",
    "                                     model.state: init_state, \n",
    "                                     model.lr: cur_lr}\n",
    "                    )\n",
    "                    init_state = last_state                    \n",
    "                    epoch_loss.append(cur_loss)\n",
    "\n",
    "                epoch_loss = np.mean(np.asarray(epoch_loss, dtype='float32'))\n",
    "                train_loss_hist.append(epoch_loss)\n",
    "\n",
    "            # evaluation\n",
    "            epoch_loss = []\n",
    "            dataset.set_batch_size(test_batch)\n",
    "            dataset.set_mode('valid')\n",
    "\n",
    "            with tf.name_scope('valid'):\n",
    "                init_state = np.zeros([3, 2, test_batch, lstm_dim], dtype = 'float32')\n",
    "                while dataset.iter_flag():\n",
    "                    \n",
    "                    batch_x, batch_y = dataset.get_data()\n",
    "                    cur_loss, last_state = sess.run(\n",
    "                        [model.loss ,model.last_state],\n",
    "                        feed_dict={model.x: batch_x,\n",
    "                                   model.y: batch_y,\n",
    "                                   model.state: init_state,\n",
    "                                   model.phase: eval_phase}\n",
    "                    )\n",
    "                    init_state = last_state\n",
    "                    epoch_loss.append(cur_loss)\n",
    "                    continue\n",
    "\n",
    "                epoch_loss = np.mean(np.asarray(epoch_loss, dtype='float32'))\n",
    "                valid_loss_hist.append(epoch_loss)\n",
    "\n",
    "            #early stopping\n",
    "            if epoch_loss >= best_valid_loss:\n",
    "                if cur_patience == patience:\n",
    "                    if cur_decay_time == decay_time:\n",
    "                        status = 'end_train'\n",
    "                    else:\n",
    "                        status = 'change_lr'\n",
    "                else:\n",
    "                    status = 'keep_train'\n",
    "            else:\n",
    "                status = 'save_param'\n",
    "                best_valid_loss = epoch_loss\n",
    "\n",
    "            end_time = time.time()\n",
    "\n",
    "\n",
    "            print('train loss - ', train_loss_hist[-1], ' | ppl - ', np.exp(train_loss_hist[-1]))\n",
    "            print('valid loss - ', valid_loss_hist[-1], ' | ppl - ', np.exp(valid_loss_hist[-1]))\n",
    "            print('status : ', status, ', training time : ', end_time - start_time)\n",
    "            epoch+=1\n",
    "\n",
    "\n",
    "        #final test\n",
    "        # evaluation\n",
    "        epoch_loss = []\n",
    "        dataset.set_batch_size(test_batch)\n",
    "        dataset.set_mode('test')\n",
    "        start_time = time.time()\n",
    "        with tf.name_scope('test'):\n",
    "            init_state = np.zeros([3, 2, test_batch, lstm_dim], dtype = 'float32')\n",
    "            while dataset.iter_flag():\n",
    "                batch_x, batch_y = dataset.get_data()\n",
    "                cur_loss, last_state = sess.run(\n",
    "                    [model.loss, model.last_state],\n",
    "                    feed_dict={model.x: batch_x,\n",
    "                               model.y: batch_y,\n",
    "                               model.state: init_state,\n",
    "                               model.phase: eval_phase}\n",
    "                )\n",
    "                init_state = last_state\n",
    "                epoch_loss.append(cur_loss)\n",
    "            epoch_loss = np.mean(np.asarray(epoch_loss, dtype='float32'))\n",
    "\n",
    "        end_time = time.time()\n",
    "        print('\\n\\n--------final result for test data--------')\n",
    "        print('loss - ', epoch_loss, ' | ppl - ', np.exp(epoch_loss))\n",
    "        print('test set inference time : ', end_time - start_time)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate \n",
    "\n",
    "#initial 5 words\n",
    "init_words = \"the company said a word\"\n",
    "word_to_id = dataset.word_to_id\n",
    "id_to_word = dataset.id_to_word\n",
    "init_ids = []\n",
    "for i in range(5):\n",
    "    init_ids.append(len(init_words))\n",
    "gen_words = init_words\n",
    "gen_ids = init_ids\n",
    "\n",
    "init_state = np.zeros([3, 2, 1, lstm_dim], dtype = 'float32')\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    model = ptb_wlm(\n",
    "            name = model_name,\n",
    "            seq_len = 1,\n",
    "            LSTM_dim = lstm_dim,\n",
    "            dropout_ratio = dr_ratio,\n",
    "            )\n",
    "    #set gpu usage to allow_growth\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    \n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        model.saver.restore(sess, result_file)\n",
    "        for i in range(5):\n",
    "            cur_x = np.reshape(np.asarray([init_ids[i]], dtype = 'int32'), [1,1])\n",
    "            cur_predict, last_state = sess.run([model.predict, model.last_state],\n",
    "                    feed_dict = {\n",
    "                        model.x : cur_x,\n",
    "                        model.state: init_state,\n",
    "                        model.phase: eval_phase\n",
    "                        }\n",
    "                    )\n",
    "            init_state = last_state\n",
    "            \n",
    "            print(id_to_word[cur_predict[0][0]])\n",
    "        gen_ids.append(cur_predict[0][0])\n",
    "        gen_words = gen_words + ' ' + id_to_word[gen_ids[-1]]\n",
    "        \n",
    "        counter = len(init_words)\n",
    "        while counter < 50:\n",
    "            cur_x = np.reshape(np.asarray([gen_ids[-1]], dtype = 'int32'), [1,1])\n",
    "            cur_predict, last_state = sess.run([model.predict, model.last_state],\n",
    "                    feed_dict = {\n",
    "                        model.x : cur_x,\n",
    "                        model.state: init_state,\n",
    "                        model.phase: eval_phase\n",
    "                        }\n",
    "                    )\n",
    "            init_state = last_state\n",
    "            gen_ids.append(cur_predict[0][0])\n",
    "            gen_words = gen_words + ' ' + id_to_word[gen_ids[-1]]\n",
    "            counter+=1\n",
    "\n",
    "        print('-----generated ids-----')\n",
    "        print(gen_ids)\n",
    "        print('-----generated sentences-----')\n",
    "        print(gen_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
